# 图神经网络训练框架与分布式训练相关文献阅读
## 文献阅读
* Deep Graph Library: Towards Efficient and Scalable Deep Learning on Graphs （[ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019](https://www.researchgate.net/publication/335617788_Deep_Graph_Library_Towards_Efficient_and_Scalable_Deep_Learning_on_Graphs)）
流行的GNN训练框架DGL，使用消息传播范式，采用了消息融合技术
[详细阅读](./DGL笔记.md)
---
* Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks （[KDD'19](https://www.researchgate.net/publication/334717498_Cluster-GCN_An_Efficient_Algorithm_for_Training_Deep_and_Large_Graph_Convolutional_Networks)）
用于大规模图的卷积GCN训练，使用vanilla mini-SGD,子图划分，将整个图分成cluster然后分别作GCN。针对GNN本身结构的优化，如果把问题方向限定在用分布式集群进行高效的GNN训练（不配适各种模型），比较合适
---
* DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks ([intel](https://www.researchgate.net/publication/350875953_DistGNN_Scalable_Distributed_Training_for_Large-Scale_Graph_Neural_Networks))
使用DGL进行分布式训练（有待详细阅读）
---
* HyGCN: A GCN Accelerator with Hybrid Architecture ([HPCA'2020](https://www.researchgate.net/publication/340696422_HyGCN_A_GCN_Accelerator_with_Hybrid_Architecture))
GCN异构芯片，可借鉴其思想（有待细读）
---

# 总结：
## 图神经网络模型
发掘结构体征
* 深度学习
    * GCN：适用于半监督、无监督，不具备繁华能力
    * GraphSAGE
* PageRank等算法，知识图谱、社区发掘等社会计算领域问题
* 思考：是否可以探索新的GNN模型来使之便于在集群上进行训练，如Cluster-GCN，但是这样的话问题变成了部署高效的针对某一种GNN模型，而不是对GNN训练的底层架构的优化

## 探索方向：
* 底层优化：
合理的稀疏矩阵算法（主要的稀疏矩阵的乘法），主要是对SpMM、SDDMM优化，但是对于大图，邻接矩阵$A$本身就很大。此时是否可以具备并行或者分布式计算的必要？
* operator的定义，对现有的node-wise、edge-wise的运算进行拆分、融合
* operator分布式部署，
    * 并行：子图划分，分别进行计算等思路，问题在于可能会损坏GNN的图结构，DGL的方法是对图采样得到子图，进行分发
    * 分布式：
* 云上部署，将GNN的训练或者部署分发部署到云端？

一些问题：
GNN训练的瓶颈：每个epoch都需要全部节点的特征(在GCN中)，存储上的问题较大
**使用子图划分的问题**：子图中的节点本来就具有一定的相似性，大概率标签相同，是否子图间的边（割集）上的信息更加重要