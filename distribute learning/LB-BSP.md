主要影响是掉队者，通过在同步的barrier处调整batch大小，来使得worker的负载适应其实时处理能力。

概括的总体思想：在iteration boundary的地方调整每个worker的batch大小来调整载荷。BSP通过推测性的在同步屏障处调整batch大小来平衡worker的batch处理时间，如何调整batch大小在CPU和GPU平台上面临着不同问题：
* CPU集群中：批处理时间与批大小成正比，系数表示处理能力，在共享设备中这个系数随时间会不断变化，应该在每个iteration开始前进行预测。用NARX(神经网络)根据CPU和内存等资源预测性能
* 一个GPU通常专注一个worker，但是GPU的batch运行时间和大小在runtime不成比例且难以预测，因此不采用基于profile的分析方法，而是提出一种迭代batch size调整算法，可以在不需要先验知识的情况下逼近负载均衡。

但是会导致batch size不均匀，因此提出了权值参数聚合，在聚合时将batch size作为梯度的权值。模型完成与样本处理量之间的关系也无法确定。

模型训练需要：1）硬件效率(多久完成一个iteration)2）统计效率(多少个iteration收敛)

两种掉队者：
* Non-deterministic straggler，来自系统抖动、垃圾回收等原因，危害性，消除快
* Deterministic straggler，源于worker的质量、数量的结构性问题，在**非专用non-dedicated系统中**长期存在
确定落队者常在非专用集群中产生，造成同步时的等待，引起低效率问题

BSP Bulk Synchronous Parallel

以往的策略：
* 通过松弛同步来绕开掉队者
    * ASP: ASynchronous Parallel，异步执行，浪费小但是统计效率低，使用陈旧stale参数计算梯度，收敛慢
    * SSP：当参数陈旧到达一定阈值后才等待，在同质化集群中有效，在异质化的非专用集群中快的worker总要停下等待，硬件效率低
* 通过冗余执行减少掉队者Mitigating Stragglers by Redundant Execution
    * 冗余执行在mapreduce和spark中广泛使用，但是只解决了最坏情况的落队者，并且浪费了额外的资源
* 通过负载均衡
    * 前述两种策略没有尝试发现落队者，而非专用网络的问题就在于资源负载不匹配问题
    * 静态负载均衡：为worker分配了常数，不需要实时监测或者通信，无法对资源变换做出反应
    * 动态负载均衡：work-stealing work-shedding算法，通常用于多核和HPC之类的任务并行模型。监控运行慢的worker将样本迁移到运行快的worker上。但是会带来监控和通信等开销，不适用于资源密集的情况。FlexRR通过worker group来发现落队者并迁移，达到次优的负载均衡

ML training属于计算密集型资源，一般用CPU并行。一般来说框架将样本batch包成一个张量矩阵同步执行，所以很衡量难细粒度的工作进程，动态调整很困难。

半动态负载均衡：
目标是：兼容已有框架；实时监控worker执行情况并动态调整负载；不干涉迭代中的常规工作，轻量化或避免数据迁移
设计哲学：
* 每次迭代用相同的计算图，因此最近的迭代中的执行状态具有参考价值，避免测量
* BSP在迭代最后进行同步可以检测所有worker的状态
* batch-size可以用于调整worker的负载同时避免通信问题

**CPU集群**
* 传统机器学习如SVM，LR等
* 非紧急任务，大公司在CPU服务器的非高峰时段训练
RNN和统计学方法无法兼顾底层的驱动资源(与计算速度紧密相关)，因此使用了NARX，本质上是一个拓展的循环神经网络。CPU集群的通信时间几乎可以忽视，用预测出来的速度归一化之后来等比分配batch-size，通过旧的计算速度，CPU使用，内存使用作为输入

**GPU集群**
* 通信时间：比CPU快若干数量级，通信时间不可忽视
* GPU加载负荷：即使batch size为0，GPU计算时间仍然大于0，需要进行与CPU交换参数，加载处理内核
* GPU饱和，对于Tesla V100之类的高端CPU，在一定batch size大小之下的计算时间几乎是常数，batch size太小会造成浪费
* LB-BSP的挑战：1）GPU的技术共享有难度很少见，CPU、内存、网络等预分配资源很少有波动，所有又没必要实时预测GPU的表现 2）静态的预测带来编程和时间开销，对于ML任何随时可能迁移的共享的GPU集群很不方便3）batch处理时间和batch线性相关，与高达数秒的iteration时间相比，任务迁移时间很短，连续训练中GPU的表现足够稳定。所以使用数值逼近的方法。**时间与batch-size非线性，单调递增**。

在GPU集群中用一种插入的算法迭代，使各个worker之间处理时间差距最小。在找出leader和straggler，如果窗口时间(增强鲁棒性)内leader、
都领先落后者，则前者增加step-size，后者减少，如果减少为0则不参加训练。如果在之前的某次迭代中leader比straggler慢则扩大窗口减少step-size

**带权重参数聚合**
一般更新参数是用的是各个batch的平均梯度，这样会偏向于小的batch。才用带权聚合使得每个样本被同等采纳

**数据访存**
只讨论了可以本地或者远程访问全部数据的情况。有的时候worker只访存本地的分区，而处理快的worker使用更大的batch，样本在快的worker上访问更频繁，这样的不均匀样本分别可能导致准确率下降，特别是分区没有很好shuffle的时候。采用SSP风格的数据调度，当一个分区的遍历时间超过阈值则在后台将数据转移一部分到更快的worker。